defaults:
  logdir: null
  traindir: null
  evaldir: null
  offline_traindir: ''
  offline_evaldir: ''
  seed: 0
  steps: 1e7  
  eval_every: 1e4
  log_every: 1e4
  reset_every: 0
  #gpu_growth: True
  device: 'cuda:0'
  precision: 16
  debug: False
  expl_gifs: False

  # Environment
  task: 'dmc_walker_walk'
  size: [64, 64]
  envs: 1
  action_repeat: 2 #2
  time_limit: 1000
  grayscale: False
  prefill: 2500
  eval_noise: 0.0
  clip_rewards: 'identity'

  # Model
  dyn_cell: 'gru'
  dyn_hidden: 200
  dyn_deter: 200
  dyn_stoch: 50
  dyn_discrete: 0
  dyn_input_layers: 1
  dyn_output_layers: 1
  dyn_rec_depth: 1
  dyn_shared: False
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  dyn_temp_post: True
  grad_heads: ['image', 'reward']
  units: 400
  reward_layers: 2
  discount_layers: 3
  value_layers: 3
  actor_layers: 4
  act: 'ELU'
  cnn_depth: 32
  encoder_kernels: [4, 4, 4, 4]
  decoder_kernels: [5, 5, 6, 6]
  decoder_thin: True
  value_head: 'normal'
  kl_scale: '1.0'
  kl_balance: '0.8'
  kl_free: '1.0'
  kl_forward: False
  pred_discount: False
  discount_scale: 1.0
  reward_scale: 1.0
  weight_decay: 0.0

  # Training
  batch_size: 50
  batch_length: 50
  train_every: 5
  train_steps: 1
  pretrain: 100
  model_lr: 3e-4
  value_lr: 8e-5
  actor_lr: 8e-5
  opt_eps: 1e-5
  grad_clip: 100
  value_grad_clip: 100
  actor_grad_clip: 100
  dataset_size: 0
  oversample_ends: False
  slow_value_target: True
  slow_actor_target: True
  slow_target_update: 100
  slow_target_fraction: 1
  opt: 'adam'

  # Behavior.
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: '0.1'
  imag_sample: True
  actor_dist: 'trunc_normal'
  actor_entropy: '1e-4'
  actor_state_entropy: 0.0
  actor_init_std: 1.0
  actor_min_std: 0.1
  actor_disc: 5
  actor_temp: 0.1
  actor_outscale: 0.0
  expl_amount: 0.0
  eval_state_mean: False
  collect_dyn_sample: True
  behavior_stop_grad: True
  value_decay: 0.0
  future_entropy: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False

sgym:

  # General
  task: 'SafetyPointCircle0-v0'
  steps: 1e7
  eval_every: 1e4
  log_every: 1e4
  prefill: 2500
  dataset_size: 0
  pretrain: 100

  # Environment
  time_limit: 1000
  action_repeat: 2
  train_every: 5
  train_steps: 1

  # Model
  grad_heads: ['image', 'reward','cost']
  dyn_cell: 'gru_layer_norm'
  pred_discount: False
  cnn_depth: 32
  dyn_deter: 200
  dyn_stoch: 50
  dyn_discrete: 0
  reward_layers: 2
  cost_layers : 2
  discount_layers: 3
  value_layers: 3
  actor_layers: 4

  # Behavior

  actor_dist: 'trunc_normal'
  expl_amount: 0.0
  actor_entropy: '1e-4'
  discount: 0.99
  imag_gradient: 'dynamics'
  imag_gradient_mix: 1.0

  # Training
  reward_scale: 2
  weight_decay: 0.0
  model_lr: 3e-4
  value_lr: 8e-5
  actor_lr: 8e-5
  safe_actor_lr: 8e-5 #8e-5
  opt_eps: 1e-5
  kl_free: '1.0'
  kl_scale: '1.0'

  #cmdp
  solve_cmdp: True

  #Cost model parameters
  cost_value_lr: 8e-5
  cost_scale: 2
  clip_costs: 'identity'

  #Multi agent paramters
  use_single_roll_out: False
  safety_look_ahead_steps: 15 #2 -primr 7 short 3
  cost_threshold: 2 #1 prime 3 short 1
  actor_behavior_scale: 0.8 # 100, 1.5 0.8
  behavior_cloning: '' #kl1, kl2, log_prob discriminator discriminator_log mse
  min_behavior_loss: 0.2 # Adjust this value as needed
  clamp_behavior_loss: False
  alpha1: 3.0 #coeff
  safe_actor_entropy: 5e-4

  # cost_imag_horizon: 5 

  #Observation setting
  ontop: False
  camera_name: 'fixednear' # fixedfar fixednear

  perturb_cost_entropy: True # to include uncertainty in the computation of cost

  cost_imag_gradient: 'dynamics' #dynamics , reinforce, mix
  cost_imag_gradient_mix: 0.05

  #explore task switching
  safe_decay_start: 1_000 #10_000
  safe_signal_prob: 1.0
  safe_signal_prob_decay: 3_000 # 90_000 
  safe_signal_prob_decay_min: 0.01

  # cost curriculum learning parmaters
  limit_decay_start: 50_000 # last value: 5_000  30_000
  limit_decay_freq: 10_000 #10_000
  limit_signal_prob: 80 #100
  limit_signal_prob_decay: 800 #1450 circle
  limit_signal_prob_decay_min:  25
  decay_cost: False

  #lagrange multipler  parameters
  cost_limit: 50
  sigmoid_a: 0.09
  lamda_projection: 'relu' # stretched_sigmoid relu
  update_lagrange_metric: 'mean_ep_cost' # target_mean target_max mean_ep_cost
  lagrangian_multiplier_init: 0.7 # 0.3
  lambda_lr: 0.0035  # 0.035 0.0035
  learnable_lagrange: False #default True PID False
  lagrangian_multiplier_fixed: 0.15 # 0.3
  max_lagrangian : 0.75 # 10 important if using stretched sigmoid PID = 0.75
  min_lagrangian: 0.1 #0.5 for curr

  #discrimiator parameters
  discriminator_layers: 2
  discriminator_units: 200
  discrimiator_lr: 1e-3
  discriminator_grad_clip: 100
  learn_discriminator: True


  #pid params
  use_pid: True


debug:

  debug: True
  pretrain: 1
  prefill: 1
  train_steps: 1
  batch_size: 10
  batch_length: 20

